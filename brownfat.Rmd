---
title: "brownfat"
output: html_document
---

```{r}
library(readxl)
library(dplyr)
library(MASS)
library(MPV)
```

## DATA CLEANING

Read the file:

```{r}
# the excel sheet represents NA values by "NA"
brownfat <- read_excel("./description/BrownFat.xls", na = "NA")
nrow(brownfat)
```

Check if there's any NA values:

```{r}
x <- colSums(is.na(brownfat)) 
x[x > 0]
```

3 variables has NA values detected: Cancer_Status, Cancer_Type, and TSH. Data cleaning needed.

Notice that $4425$ out of $n=4842$ observations had no data on their `TSH`, as this ratio is too large in order for us to confidently assign the most-probable values, we will choose to exclude the variable `TSH` altogether.

```{r}
brownfat_cleaned_data <- brownfat %>% dplyr::select(-c("TSH"))
```

There are observations where both the `Cancer_Status` and `Cancer_Type` is set to NA, as we don't have domain knowledge to assign appropriate values nor is it the main focus of this research, we will simply exclude these observations.

```{r}
# determine how many obs. display such behaviour
x <- sum(is.na(brownfat$Cancer_Status) & is.na(brownfat$Cancer_Type))
paste("No. of observations w/ Cancer_Status == Cancer_Type == NA is:", x)

# exclude these obs.
brownfat_cleaned_data <- brownfat_cleaned_data %>% 
  filter(!(is.na(Cancer_Status) & is.na(Cancer_Type)))
```

Observe that since the remaining $369  - 250 = 119$ observations still having `Cancer_Type == NA` also have that `Cancer_Status == 0`, meaning they do have cancer (thus setting `Cancer_Type` to $0$ is not a valid option) and hence we can either assign values to these missing values or once again, simply remove them.

-   Option 1: We calculate the estimate probability of each category (other than $0$) based on their frequency in the given data, and then assign `NA` values using our new probability measure.

```{r}
set.seed("14")

# calculate the estimated prob. of each category (except 0, and NAs)
cancer_status_category_prob <- brownfat_cleaned_data %>%
  filter(!is.na(Cancer_Type) & Cancer_Type != 0) %>%
  count(Cancer_Type) %>% 
  mutate(probability = n / sum(n))

# function to re-assign those w/ NA values
assign_cancer_type <- function(x) {
  if (is.na(x)) {
    sample(cancer_status_category_prob$Cancer_Type, 1, 
           prob = cancer_status_category_prob$probability)
  } else {
    x
  }
}


# re-assign those w/ Cancer_Type == NA
brownfat_cleaned_data_v1 <- brownfat_cleaned_data
brownfat_cleaned_data_v1$Cancer_Type <- sapply(brownfat_cleaned_data$Cancer_Type,
                                   assign_cancer_type)
```

Why may this method be problematic? =\> We've only taking into the account the frequencies of each of these types of cancer when in reality, a multitude of other factors affect the diagnosis of single patient. (We may consider to improve this later on by also creating a prediction model)

-   Option 2: To avoid potential hazardous assumptions, we may opt to exclude these `NA` values altogether:

```{r}
brownfat_cleaned_data_v2 <- brownfat_cleaned_data %>%
  filter(!is.na(Cancer_Type))
```

## MODEL SELECTION

Due to the majority of the data having `Total_vol` as $0$, we will built a sort of 2-step model, where the first thing to determine is which of 2 groups does the data point fall under: `Total_vol` is $0$ or $>0$. If it belong to the latter group, we predict the volume using a regression function built on training_data w/ `Total_vol` being positive.

First, we shall build the model to determine which of the 2 groups does the data point belong to.

```{r}
# make Y categorical
brownfat_cleaned_data_v2$BrownFat <- ifelse(brownfat_cleaned_data_v2$Total_vol > 0,
                                            1, 0)
# convert to factors
brownfat_cleaned_data_v2$Month <- as.factor(brownfat_cleaned_data_v2$Month)
brownfat_cleaned_data_v2$Cancer_Status <- as.factor(brownfat_cleaned_data_v2$Cancer_Status)
brownfat_cleaned_data_v2$Cancer_Type <- as.factor(brownfat_cleaned_data_v2$Cancer_Type)
```

We will partition the cleaned data (version 2) into 2 equal parts: training and testing dataset. (70/30)

```{r}
set.seed(14)
training_data <- brownfat_cleaned_data_v2 %>%
  slice_sample(prop = 0.7)
testing_data <- anti_join(brownfat_cleaned_data_v2, training_data)

# consider the training data >0 brownfat
training_data_w_bf <- training_data %>%
  filter(BrownFat > 0)
```

Next, we shall perform Step-wise Regression.

```{r}
fit_full <- lm(BrownFat ~ . -Id -Total_vol , data=training_data) # full model
fit_intercept <- lm(BrownFat ~ 1, data=training_data) # intercept model

step <- stepAIC(fit_intercept, 
                scope=list(upper=fit_full, lower=fit_intercept),
                direction="both", trace=0)
```

```{r}
summary(step)
step$anova
```

So:

```{r}
fit_final <- lm(BrownFat ~ Age + Ext_Temp + Diabetes + Sex + Weigth + Season, 
                data=training_data)
summary(fit_final)
```
Now we fit a model to predict the actual volume of brown fat (when it is not 0):
```{r}
fit_full_w_bf <- lm(Total_vol ~ . -Id -BrownFat , data=training_data_w_bf) # full model
fit_intercept_w_bf <- lm(Total_vol ~ 1, data=training_data_w_bf) # intercept model

step_w_bf <- stepAIC(fit_intercept_w_bf, 
                scope=list(upper=fit_full_w_bf, lower=fit_intercept_w_bf),
                direction="both", trace=0)
```

Now we observe the results:
```{r}
summary(step_w_bf)
step_w_bf$anova
```

Thus, the final model is:
```{r}
fit_final_w_bf <- lm(Total_vol ~ `7D_Temp` + `3D_Temp` + Cancer_Status 
                     + Ext_Temp + Glycemy + Sex + Age,
                     data=training_data_w_bf)
summary(fit_final_w_bf)
extractAIC(fit_final_w_bf)
```

Notice that the p-value for the intercept is quite high, consider the following model w/o intercept:
```{r}
fit_final_w_bf <- lm(Total_vol ~ `7D_Temp` + `3D_Temp` + Cancer_Status + 
    Ext_Temp + Glycemy + Sex + Age - 1,
                     data=training_data_w_bf)
summary(fit_final_w_bf)
extractAIC(fit_final_w_bf)
```

Note the strong increase in the adjusted $R^2$ (more than tripled), also AIC slightly decreases.

## MODEL DIAGNOSTICs

```{r}
qqnorm(fit_final_w_bf$residuals)
qqline(fit_final_w_bf$residuals)
shapiro.test(fit_final_w_bf$residuals)
```

As normality is violated, we shall perform a Box-Cox transformation.
```{r}
result <- boxcox(fit_final_w_bf)
lambda <- result$x[which.max(result$y)]
paste("As lambda=", lambda, "is close to 0, we instead take the ln transformation")
```

We re-fit the model as follows:
```{r}
fit_final_w_bf <- lm(log(Total_vol) ~ `7D_Temp` + `3D_Temp` + Cancer_Status 
                     + Ext_Temp + Glycemy + Sex + Age - 1,
                     data=training_data_w_bf)
summary(fit_final_w_bf)
extractAIC(fit_final_w_bf)
```

While adjusted $R^2$ and AIC are at an all time high and low respectively, a lot of the covariates now have quite high p-values. Because of this, we'll now turn to the interaction model.

## INTERACTION MODEL

```{r}
# center all quantitative vars to avoid multi-collinearity
# training_data_w_bf$Age = training_data_w_bf$Age - mean(training_data_w_bf$Age)
# training_data_w_bf$Day = training_data_w_bf$Day - mean(training_data_w_bf$Day)
# training_data_w_bf$Ext_Temp = training_data_w_bf$Ext_Temp - mean(training_data_w_bf$Ext_Temp)
# training_data_w_bf$`2D_Temp` = training_data_w_bf$`2D_Temp` - mean(training_data_w_bf$`2D_Temp`)
# training_data_w_bf$`3D_Temp` = training_data_w_bf$`3D_Temp` - mean(training_data_w_bf$`3D_Temp`)
# training_data_w_bf$`7D_Temp` = training_data_w_bf$`7D_Temp` - mean(training_data_w_bf$`7D_Temp`)
# training_data_w_bf$`1M_Temp` = training_data_w_bf$`1M_Temp` - mean(training_data_w_bf$`1M_Temp`)
# training_data_w_bf$Duration_Sunshine = training_data_w_bf$Duration_Sunshine - mean(training_data_w_bf$Duration_Sunshine)
# training_data_w_bf$Weigth = training_data_w_bf$Weigth - mean(training_data_w_bf$Weigth)
# training_data_w_bf$Size = training_data_w_bf$Size - mean(training_data_w_bf$Size)
# training_data_w_bf$BMI = training_data_w_bf$BMI - mean(training_data_w_bf$BMI)
# training_data_w_bf$Glycemy = training_data_w_bf$Glycemy - mean(training_data_w_bf$Glycemy)
# training_data_w_bf$LBW = training_data_w_bf$LBW - mean(training_data_w_bf$LBW)

fit_full_w_bf_inter <- lm(Total_vol ~ (. -Id -BrownFat)^2 , data=training_data_w_bf) # full model
fit_intercept_w_bf_inter <- lm(Total_vol ~ 1, data=training_data_w_bf) # intercept model

step_w_bf_inter <- stepAIC(fit_intercept_w_bf, 
                scope=list(upper=fit_full_w_bf_inter, lower=fit_intercept_w_bf_inter),
                direction="both", trace=0)
```

```{r}
summary(step_w_bf_inter)
```

The final inter. model by step-wise regression is given by:
```{r}
fit_final_w_bf_inter <- lm(Total_vol ~ `7D_Temp` + `3D_Temp` + Cancer_Status + 
    Ext_Temp + Glycemy + Sex + `7D_Temp`:Ext_Temp,
    data = training_data_w_bf)
summary(fit_final_w_bf_inter)
extractAIC(fit_final_w_bf_inter)
```
Once again, we shall remove the intercept, as well as from the diagnostics done previously, we shall take log base $e$ transformation on the response variable:

```{r}
fit_final_w_bf_inter <- lm(log(Total_vol) ~ `7D_Temp` + `3D_Temp` + Cancer_Status + 
    Ext_Temp + Glycemy + Sex + `7D_Temp`:Ext_Temp - 1, 
    data = training_data_w_bf)
summary(fit_final_w_bf_inter)
```
Observe that the p-values for the interaction model is much smaller overall compared to the original model created. However, there are some covariates we may consider removing:

- `3D_Temp`: This is mostly likely because of quadratic term `7D_Temp*3D_Temp` and as well as their high correlation, making this variable insignificant for prediction.
- `Sex`: While men and women do have slightly varying levels of brown fat (if possessed) as saw in the EDA section, this difference is not significant, though, this may be a result of the training on a small dataset.
- `Ext_Temp`: Because of high correlation with `3D_Temp` as well as `7D_Temp`.
- `Glycemy`: High p-value.

This is the final interaction model after removing some (possibly-)insignifcant predictors.

```{r}
fit_final_w_bf_inter_trimmed <- lm(log(Total_vol) ~ `7D_Temp` + Cancer_Status + `7D_Temp`:Ext_Temp - 1, data = training_data_w_bf)
summary(fit_final_w_bf_inter_trimmed)
extractAIC(fit_final_w_bf_inter_trimmed)
```
As a result, all p-values for the remaining predictors are now all quite low. However also note the low no. of covariates, this could lead to bad generalization in larger data-sets. Also that the interaction model is not that much better than the original linear model.

NOTE: PERFORM WLS (much better results)


## MODEL VALIDATION

The classification model is bad, none of the obs. would get classified as having brown fat, using a threshold of `0.5`. If we adjust the threshold to `sum(training_data$BrownFat == 1) / nrow(training_data) = 0.067` or roughly 0.7, we get much better results.

```{r}
# threshold of 0.5
predicted_test <- predict(fit_final, testing_data)
predicted_test <- ifelse(predicted_test >= 0.5, 1, 0)
actual <- testing_data$BrownFat
sum(predicted_test == 1 & actual == 1) / sum(actual)
sum(predicted_test == 1)

# threshold of 0.067 approx 0.07, for testing dataset
predicted_test <- predict(fit_final, testing_data)
predicted_test <- ifelse(predicted_test >= 0.07, 1, 0)
actual <- testing_data$BrownFat
sum(predicted_test == 1 & actual == 1) / sum(actual)
sum(predicted_test == 1)

# threshold of 0.067 approx 0.07, for training dataset
predicted_train <- predict(fit_final, training_data)
predicted_train <- ifelse(predicted_train >= 0.07, 1, 0)
actual <- training_data$BrownFat
sum(predicted_train == 1 & actual == 1) / sum(actual == 1)
sum(predicted_train == 1)
```

We will try a RF implementation.

```{r}
library(randomForest)
# Convert BrownFat to a factor for classification
training_data$BrownFat <- as.factor(training_data$BrownFat)


training_data_cpy <- training_data %>%
  dplyr::rename(TwoD_Temp = `2D_Temp`,
                ThreeD_Temp = `3D_Temp`,
                SevenD_Temp = `7D_Temp`,
                OneM_Temp = `1M_Temp`)


# Train the Random Forest model
library(ROSE) # for over/under sampling
over <- ovun.sample(BrownFat ~ . -Id -Total_vol, data=training_data_cpy,
                    method = "over", N = table(training_data_cpy$BrownFat)[1]*2, seed = 222)$data

fit_rf <- randomForest(BrownFat ~ . -Id -Total_vol, ntree = 100
                       , data=over, mtry=5, nodesize = 10, maxnodes = 15)
fit_rf
```

```{r}
testing_data_cpy <- testing_data %>%
  dplyr::rename(TwoD_Temp = `2D_Temp`,
                ThreeD_Temp = `3D_Temp`,
                SevenD_Temp = `7D_Temp`,
                OneM_Temp = `1M_Temp`)
predicted_rf <- factor(predict(fit_rf, testing_data_cpy), levels = c(0, 1)) 
actual <- testing_data_cpy$BrownFat
confusionMatrix(predicted_rf, reference = factor(actual, levels =c(0,1)))
```

Now, we shall evaluate the performance of our regression model.

```{r}
predicted_rf <- as.numeric(predicted_rf) - 1

# for testing dataset
for (i in 1:nrow(testing_data)) {
  if (predicted[i] == 1) {
    predicted_rf[i] <- predict(fit_final_w_bf_inter_trimmed, testing_data[i,])
  }
}
actual_vol <- testing_data_cpy$Total_vol

mspe <- mean((actual_vol - predicted_rf)^2)
mspe

# for training
predicted_rf <- factor(predict(fit_rf, training_data_cpy), levels = c(0, 1)) 
#actual <- training_data_cpy$BrownFat
#confusionMatrix(predicted_rf, reference = factor(actual, levels =c(0,1)))

predicted_rf <- as.numeric(predicted_rf) - 1
for (i in 1:nrow(training_data)) {
  if (predicted_rf[i] == 1) {
    predicted_rf[i] <- predict(fit_final_w_bf_inter_trimmed, training_data[i,])
  }
}
actual_vol <- training_data$Total_vol

mspe <- mean((actual_vol - predicted_rf)^2)
mspe
```
The MSPE is quite significant larger than the MSE_F, we are experience overfitting, may need to re-fit the model above.